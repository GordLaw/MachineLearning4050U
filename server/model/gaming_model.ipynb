{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2635d092",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.9.1+cu126\n",
      "True\n",
      "1\n",
      "0\n",
      "NVIDIA GeForce RTX 4060\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.__version__)\n",
    "print(torch.cuda.is_available())\n",
    "print(torch.cuda.device_count())\n",
    "print(torch.cuda.current_device())\n",
    "print(torch.cuda.get_device_name(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a10e2333",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "from pytorch_lightning import LightningModule\n",
    "from torch import Tensor\n",
    "from typing import Tuple\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from collections import OrderedDict, deque, namedtuple\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import gymnasium as gym\n",
    "from torch.utils.data.dataset import IterableDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0f1220e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PolicyNet(nn.Module):\n",
    "    def __init__(self, n_observations, n_actions, hidden_size=128):\n",
    "        super().__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(n_observations, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_size, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_size, n_actions)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "94140a06",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.0632,  0.3109, -0.5438],\n",
       "        [-0.0632,  0.3109, -0.5438]], grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = PolicyNet(8, 3, 128)\n",
    "tensor = torch.tensor([[1, 2, 3, 4, 5, 6, 7, 8],[1, 2, 3, 4, 5, 6, 7, 8]], dtype=torch.float32)\n",
    "model(tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb588546",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MemoryReplay():\n",
    "    def __init__(self, max_memory):\n",
    "        self.memory = deque(maxlen=max_memory)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.memory)\n",
    "\n",
    "    def append(self, replay): # replay is a tuple of data including, states, actions, rewards, etc\n",
    "        self.memory.append(replay)\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        return np.random.sample(self.memory, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7a2a88f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RLDataset(IterableDataset):\n",
    "    def __init__(self, buffer, sample_size=200):\n",
    "        self.buffer = buffer\n",
    "        self.sample_size = sample_size\n",
    "\n",
    "    def __iter__(self):\n",
    "        states, actions, rewards, dones, new_states = self.buffer.sample(self.sample_size)\n",
    "        for i in range(len(dones)):\n",
    "            yield states[i], actions[i], rewards[i], dones[i], new_states[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24575cf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# interacts witht the environment\n",
    "class Agent():\n",
    "    def __init__(self, env, replay_buffer):\n",
    "        self.env = env\n",
    "        self.state = self.env.reset()\n",
    "        self.replay_buffer = replay_buffer\n",
    "\n",
    "    def reset(self):\n",
    "        self.state = self.env.reset()\n",
    "\n",
    "    def get_action(self, net, epsilon):\n",
    "        if np.random.random() < epsilon:\n",
    "            action = self.env.action_space.sample() # this belongs to gym.env look into this\n",
    "        else:\n",
    "            state = torch.tensor([self.state])\n",
    "            q_values = net(state)\n",
    "            action = torch.max(q_values, dim=1)\n",
    "            action = int(action.item())\n",
    "\n",
    "        return action\n",
    "    \n",
    "    @torch.no_grad()\n",
    "    def play_step(self, net, epsilon):\n",
    "        action = self.get_action(net, epsilon)\n",
    "        \n",
    "        replay_data = self.env.step(action)\n",
    "        self.replay_buffer.append(replay_data)\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ae780a60",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyEnv(gym.Env):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "    \n",
    "    def reset(self):\n",
    "        pass\n",
    "\n",
    "    def step(self):\n",
    "        pass\n",
    "\n",
    "    def render(self):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c1caf04",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TargetNet():\n",
    "    def __init__(self, gamma, epsilon, lr, input_dim, batch_size, n_actions, memory_replay_size=1000):\n",
    "        self.gamma = gamma\n",
    "        self.epsilon = epsilon\n",
    "        self.lr = lr\n",
    "        self.batch_size = batch_size\n",
    "        self.memory_replay_size = memory_replay_size\n",
    "        # look into save_hyperparamete() function\n",
    "\n",
    "        self.policy_net = PolicyNet(input_dim, n_actions)\n",
    "        self.memory_replay = MemoryReplay(memory_replay_size)\n",
    "        self.env = MyEnv()\n",
    "        self.agent = Agent(self.env, self.memory_replay)\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        return torch.optim.Adam(self.parameters(), lr=self.lr)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.policy_net(x)\n",
    "    \n",
    "    def loss(self, batch):\n",
    "        states, actions, rewards, dones, next_states = batch # fix this\n",
    "\n",
    "        state_action_values = self.net(states).gather(1, actions.long().unsqueeze(-1)).squeeze(-1) # figure out what gather does exactly\n",
    "\n",
    "        with torch.no_grad():\n",
    "            next_state_values = self.target_net(next_states).max(1)[0]\n",
    "            next_state_values[dones] = 0.0\n",
    "            next_state_values = next_state_values.detach()\n",
    "\n",
    "        expected_state_action_values = next_state_values * self.gamma + rewards\n",
    "\n",
    "        return nn.MSELoss()(state_action_values, expected_state_action_values)\n",
    "    \n",
    "    def shared_step(self, mode:str, batch:Tuple[Tensor, Tensor], batch_index:int):\n",
    "        x, target = batch\n",
    "        output = self.forward(x)\n",
    "        loss = self.loss(output, target)\n",
    "        self.accuracy(output, target)\n",
    "        self.log(f\"{mode}_step_acc\", self.accuracy, prog_bar=True)\n",
    "        self.log(f\"{mode}_step_loss\", loss, prog_bar=False)\n",
    "        return loss\n",
    "    \n",
    "    def training_step(self, batch, batch_index):\n",
    "        return self.shared_step('train', batch, batch_index)\n",
    "    \n",
    "    def validation_step(self, batch, batch_index):\n",
    "        return self.shared_step('validation', batch, batch_index)\n",
    "    \n",
    "    def __dataloader(self):\n",
    "        dataset = RLDataset(self.memory_replay, self.memory_replay_size)\n",
    "        dataloader = DataLoader(\n",
    "            dataset=dataset,\n",
    "            batch_size=self.memory_replay_size,\n",
    "        )\n",
    "        return dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "51fcb2c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "work\n"
     ]
    }
   ],
   "source": [
    "t = TargetNet(1,2,3,4,5,6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41a62071",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv (3.12.7)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
